# ğŸ“‹ NBO Package - Complete End User Guide

## For Users Who Clone/Download the Repository

This guide shows exactly what end users need to do after getting your NBO package repository.

---

## ğŸš€ Step 1: Installation

### Prerequisites

- Python 3.8 or higher
- Git (if cloning) or download the repository as ZIP

### Get the Package

```bash
# Option A: Clone the repository
git clone <your-repository-url>
cd nbo-package

# Option B: Download ZIP and extract
# Download the ZIP file, extract it, and navigate to the folder
cd nbo-package
```

### Install the Package

```bash
# Install the package (this makes nbo-run commands available globally)
pip install -e .

# Verify installation
nbo-run --version
# Should output: nbo-package 1.0.0
```

**âœ… After this step:** All `nbo-run` commands are available globally on their system.

---

## ğŸ“‚ Step 2: Understand the Package Structure

```
nbo-package/
â”œâ”€â”€ data/                          # â† Sample data (for reference only)
â”œâ”€â”€ nbo/                           # â† Package code (don't modify)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ database_schema_v1.1.json  # â† YOUR SCHEMA (check this!)
â”‚   â”‚   â””â”€â”€ settings.json               # â† Pipeline settings
â”‚   â””â”€â”€ [Python scripts]
â”œâ”€â”€ output/                        # â† Results will go here
â””â”€â”€ [Package files]
```

---

## ğŸ” Step 3: Check the Schema Requirements

### View the Schema

```bash
# The schema file shows exactly what columns your CSV files need
# Location: nbo/config/database_schema_v1.1.json
```

**Or check it programmatically:**

```bash
# See what tables and columns are expected
nbo-run check-pipeline
```

### Create Data Templates (Recommended)

```bash
# This creates CSV templates with the exact schema requirements
nbo-run setup-data-templates --output-dir my_data

# This creates:
# my_data/
#   â”œâ”€â”€ offer_master.csv      (your offers/promotions)
#   â”œâ”€â”€ touch_history.csv     (customer communication history)
#   â”œâ”€â”€ feature_mart.csv      (customer features)
#   â”œâ”€â”€ modeling_set_v2.csv   (training data)
#   â””â”€â”€ README.md            (detailed instructions)
```

---

## ğŸ“Š Step 4: Prepare Your Data

### Required Input Files

You need to provide these CSV files (use templates as reference):

1. **`offer_master.csv`** - Your promotion catalog

   - Required columns: `promotion_id`, `promotion_name`, `product_category`, `base_price`, `start_date`, `end_date`, `legal_flag`, `channel_eligibility`

2. **`touch_history.csv`** - Customer communication history

   - Required columns: `guest_id`, `promotion_id`, `touch_ts`, `channel`

3. **`feature_mart.csv`** - Customer features and transaction history

   - Required columns: `guest_id`, `asof_date`, `aov_28d`, `aov_90d`, `aov_365d`, etc. (37+ columns)

4. **`modeling_set_v2.csv`** - Training data (optional but recommended)
   - Required columns: `guest_id`, `response_within_window`, `treatment_flag`, `train_split`, etc.

### Where to Place Your Data

```bash
# Option A: Use the templates directory
# Edit the CSV files in my_data/ with your actual data

# Option B: Create your own data directory
mkdir user_data
# Copy your CSV files to user_data/
```

---

## âœ… Step 5: Validate Your Data

### Check Data Quality

```bash
# Validate your data against the schema
nbo-run --data-path my_data validate-user-data --save-report

# This will:
# âœ“ Check all required columns exist
# âœ“ Validate data types and formats
# âœ“ Run business rule validations
# âœ“ Generate quality scores
# âœ“ Save detailed report as validation_report.json
```

### Validation Output

- âœ… **PASSED**: Data is ready for pipeline
- âš ï¸ **PARTIAL**: Some files passed, some failed
- âŒ **FAILED**: Critical issues need fixing

---

## ğŸš€ Step 6: Run the Complete Pipeline

### Execute All Steps

```bash
# Run the complete NBO pipeline
nbo-run --data-path my_data --output-path results pipeline

# This executes all 7 steps in order:
# 1. catalog_guardrails    â†’ processes your offer_master.csv
# 2. contract_checks       â†’ validates offer catalog
# 3. fatigue_candidates    â†’ generates candidates (needs feature_mart + touch_history)
# 4. model_training        â†’ trains ML models and scores offers
# 5. guardrails_winners    â†’ applies business rules and selects winners
# 6. test_marketing_view   â†’ creates marketing campaign view
# 7. shap                  â†’ adds model explanations
```

### Run Specific Steps (Optional)

```bash
# Run only certain steps
nbo-run --data-path my_data --output-path results pipeline --steps catalog_guardrails model_training

# Run individual step
nbo-run --data-path my_data --output-path results step model_training
```

---

## ğŸ“„ Step 7: Get Your Results

### Output Location

All results are saved in the `results/` directory (or whatever you specified with `--output-path`):

```
results/
â”œâ”€â”€ model_scores_output.csv        # â† All scored guest-offer combinations
â”œâ”€â”€ decision_log_output.csv        # â† Final offer decisions per customer
â”œâ”€â”€ test_marketing_view_output.csv # â† Marketing campaign view
â”œâ”€â”€ offer_catalog_v1.csv           # â† Processed offer catalog
â””â”€â”€ scored_candidates.csv          # â† Candidates after fatigue filtering
```

### Key Output Files

1. **`model_scores_output.csv`** - Complete scoring results

   - Every guest-offer combination with ML scores
   - Columns: `guest_id`, `promotion_id`, `p_treat`, `p_ctrl`, `uplift`, `eim_raw`, etc.

2. **`decision_log_output.csv`** - Final decisions (ONE offer per customer)

   - The winning offer for each customer
   - Columns: `guest_id`, `promotion_id`, `eim_final`, `why_selected`, provenance fields

3. **`test_marketing_view_output.csv`** - Marketing-ready view
   - Combined view for campaign execution
   - Includes offer details, customer info, and decision rationale

---

## ğŸ”§ Step 8: Troubleshooting & Help

### Common Commands

```bash
# Get help
nbo-run --help

# List all available pipeline steps
nbo-run list-steps

# Check if everything is configured correctly
nbo-run check-pipeline

# Validate just the data files (without schema)
nbo-run --data-path my_data validate-data
```

### Common Issues

**"Missing required files"**

```bash
# Use templates to see exactly what files you need
nbo-run setup-data-templates --output-dir templates
# Check templates/README.md for detailed requirements
```

**"Schema validation failed"**

```bash
# Check the schema file to see expected columns
# Location: nbo/config/database_schema_v1.1.json
# Or use templates to see the exact structure needed
```

**"Pipeline step failed"**

```bash
# Run steps individually to identify the problem
nbo-run --data-path my_data step catalog_guardrails
nbo-run --data-path my_data step model_training
```

---

## ğŸ“‹ Complete Workflow Summary

```bash
# 1. Get the package
git clone <repo-url> && cd nbo-package

# 2. Install
pip install -e .

# 3. Create data templates
nbo-run setup-data-templates --output-dir my_data

# 4. Edit CSV files in my_data/ with your actual data

# 5. Validate your data
nbo-run --data-path my_data validate-user-data

# 6. Run the pipeline
nbo-run --data-path my_data --output-path results pipeline

# 7. Check results in results/ directory
```

**Total time:** ~10 minutes setup + data preparation time

**Result:** Professional NBO pipeline outputs with complete ML scoring and business rule optimization! ğŸ‰

---

## ğŸ¯ What You Get

- âœ… **Complete ML Pipeline**: Uplift modeling, scoring, optimization
- âœ… **Business Rules**: Fatigue management, margin floors, discount caps
- âœ… **Data Validation**: Against your JSON schema + business rules
- âœ… **Professional Outputs**: Ready for marketing campaign execution
- âœ… **Full Provenance**: Timestamps, versions, decision rationale
- âœ… **Quality Reports**: Data quality scores and validation details
